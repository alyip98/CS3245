1) You will observe that a large portion of the terms in the dictionary are numbers. However, we normally do not use numbers as query terms to search. Do you think it is a good idea to remove these number entries from the dictionary and the postings lists? Can you propose methods to normalize these numbers? How many percentage of reduction in disk storage do you observe after removing/normalizing these numbers?


2) What do you think will happen if we remove stop words from the dictionary and postings file? How does it affect the searching phase?

Stop words account for 30%. It's removal will reduce the size of storage.

3) The NLTK tokenizer may not correctly tokenize all terms. What do you observe from the resulting terms produced by sent_tokenize() and word_tokenize()? Can you propose rules to further refine these results?

- sent_tokenize() is supposed to tokenise the given input into sentences. It seems that it identifies a new sentence by newline \n and full stop . .

- word_tokenize() is supposed to tokenise the given input into words. It is able to do it correctly for most of the time.
One thing to note is that for eg, "last year." becomes "last", "year" and "." where the punctuation mark is marked as a word.